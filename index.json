[{"authors":["adina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bfc0fd43f26e67f5f46c4bc0baf09b8e","permalink":"https://rycolab.github.io/authors/adina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/adina/","section":"authors","summary":"","tags":null,"title":"Adina Williams","type":"authors"},{"authors":["alex"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d7149a99f41440e55ea517c3fb5d3c99","permalink":"https://rycolab.github.io/authors/alex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex/","section":"authors","summary":"","tags":null,"title":"Alexander Hoyle","type":"authors"},{"authors":["clara"],"categories":null,"content":"Clara recently started her PhD with Ryan at ETH Zürich. Her research interests include neural machine translation and robustness in AI-based systems. In her free time, she likes to rock climb, trail run, and search for the elusive cheap bar in Switzerland.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f3de996f9586f710ca1ed8c97792251c","permalink":"https://rycolab.github.io/authors/clara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clara/","section":"authors","summary":"Clara recently started her PhD with Ryan at ETH Zürich. Her research interests include neural machine translation and robustness in AI-based systems. In her free time, she likes to rock climb, trail run, and search for the elusive cheap bar in Switzerland.","tags":null,"title":"Clara Meister","type":"authors"},{"authors":["edo"],"categories":null,"content":"Edoardo is a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where he is part of St John\u0026rsquo;s College. He is supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. He has previously interned as an AI/ML researcher at Apple in Cupertino, and been awarded a Google Research Faculty Award for a project co-written with his supervisor. His research focuses on few-shot multilingual learning and on the specialization of distributional word representations with structured knowledge.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0d9d7c8635f56e85ab0ca3b09a1442fb","permalink":"https://rycolab.github.io/authors/edo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/edo/","section":"authors","summary":"Edoardo is a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where he is part of St John\u0026rsquo;s College. He is supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. He has previously interned as an AI/ML researcher at Apple in Cupertino, and been awarded a Google Research Faculty Award for a project co-written with his supervisor. His research focuses on few-shot multilingual learning and on the specialization of distributional word representations with structured knowledge.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["ema"],"categories":null,"content":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research focuses on language grounding from multimodal and multilingual contexts. He also likes neural machine translation, typology and ramen.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bbe9321070c19534ca5cfe036a68e756","permalink":"https://rycolab.github.io/authors/ema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ema/","section":"authors","summary":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research focuses on language grounding from multimodal and multilingual contexts. He also likes neural machine translation, typology and ramen.","tags":null,"title":"Emanuele Bugliarello","type":"authors"},{"authors":["irene"],"categories":null,"content":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki. Her primary interests are machine learning and anything in the intersection of computer science and statistics. During her free time she likes running (away from responsibilities) and correcting people who say that Finland is part of Scandinavia.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"89af9b35ecd8250fa5b24fe3dfe7d186","permalink":"https://rycolab.github.io/authors/irene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/irene/","section":"authors","summary":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki. Her primary interests are machine learning and anything in the intersection of computer science and statistics. During her free time she likes running (away from responsibilities) and correcting people who say that Finland is part of Scandinavia.","tags":null,"title":"Irene Nikkarinen","type":"authors"},{"authors":["isabelle"],"categories":null,"content":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"76b480c4a09bc036d7176e3210f31a66","permalink":"https://rycolab.github.io/authors/isabelle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/isabelle/","section":"authors","summary":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning","tags":null,"title":"Isabelle Augenstein","type":"authors"},{"authors":["jen"],"categories":null,"content":"Jennifer is an MPhil student at the University of Cambridge, supervised by Ryan and supported by a Cambridge DeepMind Scholarship. She is interested in techniques for low-resource NLP, gender bias in NLP and annotation of linguistic data, among other things. Her background is in Algebraic Geometry, so she is always happy to get a chance to put her mathematical skills to good use. In her spare time she enjoys learning languages, reading, eating chicken katsu curry and travelling as much as possible.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5d240d57e5fd9bed1d26eb181ebe2842","permalink":"https://rycolab.github.io/authors/jen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jen/","section":"authors","summary":"Jennifer is an MPhil student at the University of Cambridge, supervised by Ryan and supported by a Cambridge DeepMind Scholarship. She is interested in techniques for low-resource NLP, gender bias in NLP and annotation of linguistic data, among other things. Her background is in Algebraic Geometry, so she is always happy to get a chance to put her mathematical skills to good use. In her spare time she enjoys learning languages, reading, eating chicken katsu curry and travelling as much as possible.","tags":null,"title":"Jennifer C. White","type":"authors"},{"authors":["jiaoda"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f5d73b305f66523f6084d93add0fbdf","permalink":"https://rycolab.github.io/authors/jiaoda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaoda/","section":"authors","summary":"","tags":null,"title":"Jiaoda Li","type":"authors"},{"authors":["tiago"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"9d3fdfe8719fded59b20776fb11d2964","permalink":"https://rycolab.github.io/authors/josef/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/josef/","section":"authors","summary":"","tags":null,"title":"Josef Valvoda","type":"authors"},{"authors":["jun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7d339d8f30d803efeaeb243a5fe6af73","permalink":"https://rycolab.github.io/authors/jun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jun/","section":"authors","summary":"","tags":null,"title":"Jun Yen Leung","type":"authors"},{"authors":["kat"],"categories":null,"content":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"77b0a8f9ddff4ab3e5c7552c687132ed","permalink":"https://rycolab.github.io/authors/kat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kat/","section":"authors","summary":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.","tags":null,"title":"Ekaterina Vylomova","type":"authors"},{"authors":["liz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fc7f296399b51eca9f04506227a81c4e","permalink":"https://rycolab.github.io/authors/liz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liz/","section":"authors","summary":"","tags":null,"title":"Elizabeth Salesky","type":"authors"},{"authors":["lucas"],"categories":null,"content":"Lucas is an MPhil student at the University of Cambridge, supervised by Ryan. He is primarily interested in the development of interpretability methods and their application to NLP models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f4eeb2051886e69b374435203391df51","permalink":"https://rycolab.github.io/authors/lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lucas/","section":"authors","summary":"Lucas is an MPhil student at the University of Cambridge, supervised by Ryan. He is primarily interested in the development of interpretability methods and their application to NLP models.","tags":null,"title":"Lucas Torroba Hennigen","type":"authors"},{"authors":["mans"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bcd9a2ac5f3d13f165b3f46243c9f942","permalink":"https://rycolab.github.io/authors/mans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mans/","section":"authors","summary":"","tags":null,"title":"Mans Hulden","type":"authors"},{"authors":["marinela"],"categories":null,"content":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen. She is a member of Trinity College and is funded by Trinity Overseas Bursary.\nBefore starting her PhD, Marinela obtained MSc and BSc degrees in Mathematics, both from the University of Belgrade in Serbia.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7cbd738aeab27bddcfebd1c698fee92d","permalink":"https://rycolab.github.io/authors/marinela/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marinela/","section":"authors","summary":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen. She is a member of Trinity College and is funded by Trinity Overseas Bursary.\nBefore starting her PhD, Marinela obtained MSc and BSc degrees in Mathematics, both from the University of Belgrade in Serbia.","tags":null,"title":"Marinela Parovic","type":"authors"},{"authors":["martina"],"categories":null,"content":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b08b9d0bebfd9788e31d72c0fa38ce6f","permalink":"https://rycolab.github.io/authors/martina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/martina/","section":"authors","summary":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.","tags":null,"title":"Martina Forster","type":"authors"},{"authors":["niklas"],"categories":null,"content":"Niklas is fascinated by interdisciplinary questions in computational social science, graph-based machine learning and network science. Prior to pursuing a PhD, he worked at the interface of these fields in industry (IBM AI Core, Microsoft Research, German Federal Foreign Office) and research (UCL, University of Oxford, Tsinghua University) environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3980a18dc1184eb009e3646396b5d2a3","permalink":"https://rycolab.github.io/authors/niklas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/niklas/","section":"authors","summary":"Niklas is fascinated by interdisciplinary questions in computational social science, graph-based machine learning and network science. Prior to pursuing a PhD, he worked at the interface of these fields in industry (IBM AI Core, Microsoft Research, German Federal Foreign Office) and research (UCL, University of Oxford, Tsinghua University) environments.","tags":null,"title":"Niklas Stoehr","type":"authors"},{"authors":["paula"],"categories":null,"content":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship. Prior to starting her PhD she did the BSc in Computer Science at the University of St Andrews and completed the MPhil in Advanced Computer Science at Cambridge. During those degrees she was generously supported by the G. D. Fahrenheit Scholarship, awarded by the City Council of Gdańsk.\nHer main research interests include cross-lingual learning and computational approaches to linguistic typology and morphology. She is fascinated by how world’s languages differ in their means of encoding meaning and hopes that investigating those differences, alongside language similarities can facilitate building better, less biased NLP systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"74eae12507749d073766d39a5b14ab4e","permalink":"https://rycolab.github.io/authors/paula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/paula/","section":"authors","summary":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship. Prior to starting her PhD she did the BSc in Computer Science at the University of St Andrews and completed the MPhil in Advanced Computer Science at Cambridge. During those degrees she was generously supported by the G. D.","tags":null,"title":"Paula Czarnowska","type":"authors"},{"authors":["ran"],"categories":null,"content":"Ran is a first year PhD student at the University of Cambridge with Ryan. He is primarily interested in research regarding algorithms, parsing, entropy computation, and gender bias in NLP systems and is picking up new interests as he goes along. While he waits for his systems to train he likes to play tennis, lacrosse and do stand-up comedy.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6eba7e88ca7bbc6ce8119664e70023ed","permalink":"https://rycolab.github.io/authors/ran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran/","section":"authors","summary":"Ran is a first year PhD student at the University of Cambridge with Ryan. He is primarily interested in research regarding algorithms, parsing, entropy computation, and gender bias in NLP systems and is picking up new interests as he goes along. While he waits for his systems to train he likes to play tennis, lacrosse and do stand-up comedy.","tags":null,"title":"Ran Zmigrod","type":"authors"},{"authors":["rowan"],"categories":null,"content":"Rowan is a masters student, now in his fourth year at the University of Cambridge, jointly supervised by Ryan and Simone Teufel. He previously worked on gender-bias mitigation from machine-learnt semantic models, and is currently working on conceptual metaphor. He, too, has things he does in his free time.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"https://rycolab.github.io/authors/rowan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rowan/","section":"authors","summary":"Rowan is a masters student, now in his fourth year at the University of Cambridge, jointly supervised by Ryan and Simone Teufel. He previously worked on gender-bias mitigation from machine-learnt semantic models, and is currently working on conceptual metaphor. He, too, has things he does in his free time.","tags":null,"title":"Rowan Hall Maudslay","type":"authors"},{"authors":["ryan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"65d4d588cef9fb8886b394cbe1e95b85","permalink":"https://rycolab.github.io/authors/ryan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ryan/","section":"authors","summary":"","tags":null,"title":"Ryan Cotterell","type":"authors"},{"authors":["shijie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"be2d1967e64e4ed029b3e9f673940c75","permalink":"https://rycolab.github.io/authors/shijie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shijie/","section":"authors","summary":"","tags":null,"title":"Shijie Wu","type":"authors"},{"authors":["simone"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"561fdecbc989c73616aba426ed25861c","permalink":"https://rycolab.github.io/authors/simone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/simone/","section":"authors","summary":"","tags":null,"title":"Simone Teufel","type":"authors"},{"authors":["stefan"],"categories":null,"content":"Stefan is an MPhil student at the University of Cambridge. His interests include explainability of deep learning models, neural machine translation, and natural language generation. He likes spending his free time with friends and whenever he does not have free time, he likes working with friends.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1fee0630ebf691a0b4eb76303184b35c","permalink":"https://rycolab.github.io/authors/stefan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stefan/","section":"authors","summary":"Stefan is an MPhil student at the University of Cambridge. His interests include explainability of deep learning models, neural machine translation, and natural language generation. He likes spending his free time with friends and whenever he does not have free time, he likes working with friends.","tags":null,"title":"Stefan Lazov","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Tiago is a first year PhD student at University of Cambridge, where he is supervised by Ryan. His primary research focus is on morphology, using information theory to study it. Tiago enjoys the few sunny days in the UK and is always looking forward to summer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fdbf9f3b62c6aa81ec87c8656829ba69","permalink":"https://rycolab.github.io/authors/tiago/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tiago/","section":"authors","summary":"Tiago is a first year PhD student at University of Cambridge, where he is supervised by Ryan. His primary research focus is on morphology, using information theory to study it. Tiago enjoys the few sunny days in the UK and is always looking forward to summer.","tags":null,"title":"Tiago Pimentel","type":"authors"},{"authors":["tim"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"07d172c41c903101879fc5085cda4837","permalink":"https://rycolab.github.io/authors/tim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tim/","section":"authors","summary":"","tags":null,"title":"Tim Vieira","type":"authors"},{"authors":["Elizabeth Salesky","Eleanor Chodroff","Tiago Pimentel","Matthew Wiesner","Ryan Cotterell","Alan W Black","Jason Eisner"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"630bf245688581fe745a2fc35fde7c29","permalink":"https://rycolab.github.io/publication/saleskyal-acl-20/","publishdate":"2020-05-05T06:05:06.652111Z","relpermalink":"/publication/saleskyal-acl-20/","section":"publication","summary":"A major hurdle in data-driven research on typology is data availability in many diverse languages. We present the first large-scale corpus for phonetic typology, with aligned phone segments and phonetic measures for vowels and sibilants in 600 languages. Extending extraction procedures to new and particularly low-resource languages is non-trivial and computationally-intensive; releasing token-level measurements enables far greater research accessibility in this area. This resource is the first of its kind and will enable investigation of phonetic typology at a much larger scale and across many languages for which previous resources do not exist. We describe the methodology to create our corpus, VoxClamantis, and illustrate research it enables through a series of case studies. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","tags":null,"title":"A Corpus for Large-Scale Phonetic Typology","type":"publication"},{"authors":["Rowan Hall Maudslay","Josef Valvoda","Tiago Pimentel","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"553515b076cfd79cf6ac3caab7b3fc42","permalink":"https://rycolab.github.io/publication/hall-maudslayal-acl-20/","publishdate":"2020-05-05T06:05:04.655959Z","relpermalink":"/publication/hall-maudslayal-acl-20/","section":"publication","summary":"Measuring what linguistic information is encoded in continuous representations of language has become a popular area of research. To do this, researchers train \"probes\"— supervised models designed to extract linguistic structure from embeddings. The line between what constitutes a probe and a model designed to achieve a particular task is often blurred. To fully understand what we are learning about the target language representation—or the instrument with which we performing measurement with for that matter—we would do well to compare probes to classic parsers. As a case study, we consider the structural probe (Hewitt and Manning, 2019), designed to quantify the presence of syntactic information. We create a simple parser that improves upon the performance of the structural probe by 11.4% on UUAS, despite having an identical lightweight parameterization. Under a second less common metric, however, the structural probe outperforms traditional parsers. This begs the question: why should some metrics be preferred for probing and others for parsing?","tags":null,"title":"A Tale of a Probe and a Parser","type":"publication"},{"authors":["Clara Meister","Elizabeth Salesky","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"65df8ea6f2757b720fbb48bafcdc38a4","permalink":"https://rycolab.github.io/publication/meisteral-acl-20/","publishdate":"2020-05-05T06:05:09.733722Z","relpermalink":"/publication/meisteral-acl-20/","section":"publication","summary":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place. Our code is available online at https://github. com/rycolab/entropyRegularization.","tags":null,"title":"Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing","type":"publication"},{"authors":["Tiago Pimentel","Josef Valvoda","Rowan Hall Maudslay","Ran Zmigrod","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"96c6d62c85e714712e9a3cd7dfcf94c8","permalink":"https://rycolab.github.io/publication/pimentelal-acl-20/","publishdate":"2020-05-05T06:05:03.471346Z","relpermalink":"/publication/pimentelal-acl-20/","section":"publication","summary":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.  A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inhering in the contextualized representation. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about both parts of speech and dependency labels, evaluating it in a set of ten typologically diverse languages often under-represented in parsing research, plus English, totalling eleven languages.  We find BERT only accounts for more information about parts of speech than a traditional type-based word embedding in five of the eleven analysed languages. When we look at dependency labels, BERT does improve upon type-based embeddings in all analysed languages, but accounting for at most 12% more information.","tags":null,"title":"Information-Theoretic Probing for Linguistic Structure","type":"publication"},{"authors":["Emanuele Bugliarello","Sabrina J. Mielke","Antonios Anastasopoulos","Ryan Cotterell","Naoaki Okazaki"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"10a46a0454ae8759bd3c1cf7f3c4f89a","permalink":"https://rycolab.github.io/publication/bugliarelloal-acl-20/","publishdate":"2020-05-05T06:05:07.793441Z","relpermalink":"/publication/bugliarelloal-acl-20/","section":"publication","summary":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","tags":null,"title":"It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information","type":"publication"},{"authors":["Adina Williams","Tiago Pimentel","Arya McCarthy","Hagen Blix","Eleanor Chodroff","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"628b1f074b060a453a0afee5af15adce","permalink":"https://rycolab.github.io/publication/williamsal-acl-20/","publishdate":"2020-05-05T06:05:05.802248Z","relpermalink":"/publication/williamsal-acl-20/","section":"publication","summary":"The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also indicative of grammatical gender \u0026mdash; which, as we quantitatively verify, can itself share information with declension class \u0026mdash; so we also control for gender. We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). The three-way interaction between class, form, and meaning (given gender) is also significant. Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages. The code is publicly available at https://github.com/rycolab/declension-mi.","tags":null,"title":"Predicting Declension Class from Form and Meaning","type":"publication"},{"authors":["Alexander Erdmann","Micha Elsner","Shijie Wu","Ryan Cotterell","Nizar Habash"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"05b7c75176d48aa04a24594196d68eb3","permalink":"https://rycolab.github.io/publication/erdmannal-acl-20/","publishdate":"2020-05-05T06:05:08.797369Z","relpermalink":"/publication/erdmannal-acl-20/","section":"publication","summary":"This work treats the paradigm discovery problem (PDP)—the task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work. Our code and data are available at https://github.com/alexerdmann/ParadigmDiscovery.","tags":null,"title":"The Paradigm Discovery Problem","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f81a6754c8d2a554ad1123f73967933b","permalink":"https://rycolab.github.io/publication/williamsal-tacl-20/","publishdate":"2020-05-05T06:05:03.007349Z","relpermalink":"/publication/williamsal-tacl-20/","section":"publication","summary":"We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relation- ships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer a deeper investiga- tion of these relationships for future work.","tags":null,"title":"On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fc0a72e5e26865b4a0609143cb55a02c","permalink":"https://rycolab.github.io/publication/pimentelal-tacl-20/","publishdate":"2020-05-05T06:05:02.585431Z","relpermalink":"/publication/pimentelal-tacl-20/","section":"publication","summary":"We present methods for calculating a measure of phonotactic complexity—bits per phoneme—that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language’s phonotactics are. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of −0.74 between bits per phoneme and the average length of words.","tags":null,"title":"Phonotactic Complexity and its Trade-offs","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Edouard Grave","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"296b08393c2f8c8422cb74edca0e8730","permalink":"https://rycolab.github.io/publication/czarnowskaal-emnlp-ijcnlp-19/","publishdate":"2020-03-13T16:20:20.22368Z","relpermalink":"/publication/czarnowskaal-emnlp-ijcnlp-19/","section":"publication","summary":"Human translators routinely have to translate rare inflections of words--due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habláramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.","tags":null,"title":"Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction","type":"publication"},{"authors":["Pei Zhou","Weijia Shi","Jieyu Zhao","Kuan-Hao Huang","Muhao Chen","Ryan Cotterell","Kai-Wei Chang"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"25f9a7f3afd4937aa991707ae9d06c12","permalink":"https://rycolab.github.io/publication/zhoual-emnlp-ijcnlp-19/","publishdate":"2020-03-13T16:20:21.128469Z","relpermalink":"/publication/zhoual-emnlp-ijcnlp-19/","section":"publication","summary":"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.","tags":null,"title":"Examining Gender Bias in Languages with Grammatical Gender","type":"publication"},{"authors":["Rowan Hall Maudslay","Hila Gonen","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"74d11d07da4859c3b2be113fbd5adbda","permalink":"https://rycolab.github.io/publication/hall-maudslayal-emnlp-ijcnlp-19/","publishdate":"2020-03-13T16:20:17.600759Z","relpermalink":"/publication/hall-maudslayal-emnlp-ijcnlp-19/","section":"publication","summary":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.","tags":null,"title":"It’s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b7f65d5b01c4b86c378d10100edac08d","permalink":"https://rycolab.github.io/publication/williamsal-emnlp-ijcnlp-19/","publishdate":"2020-03-13T16:20:18.53466Z","relpermalink":"/publication/williamsal-emnlp-ijcnlp-19/","section":"publication","summary":"Many of the world's languages employ grammatical gender on the lexeme. For instance, in Spanish, house ''casa'' is feminine, whereas the word for paper ''papel'' is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","tags":null,"title":"Quantifying the Semantic Core of Gender Systems","type":"publication"},{"authors":["Edoardo Maria Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bd8665dc8c25e8141fa3a624c5fd0955","permalink":"https://rycolab.github.io/publication/pontial-emnlp-ijcnlp-19/","publishdate":"2020-03-13T16:20:19.537574Z","relpermalink":"/publication/pontial-emnlp-ijcnlp-19/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace's method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world's languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-Shot Language Modeling","type":"publication"},{"authors":["Arya D. McCarthy","Ekaterina Vylomova","Shijie Wu","Chaitanya Malaviya","Lawrence Wolf-Sonkin","Garrett Nicolai","Christo Kirov","Miikka Silfverberg","Sabrina Mielke","Jeffrey Heinz","Ryan Cotterell","Mans Hulden"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"da6c1c8cf0e58894242c01b498a5fdd8","permalink":"https://rycolab.github.io/publication/mccarthyal-tacl-19/","publishdate":"2020-03-13T16:20:16.818736Z","relpermalink":"/publication/mccarthyal-tacl-19/","section":"publication","summary":"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.","tags":null,"title":"The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection","type":"publication"},{"authors":["Ran Zmigrod","Sabrina Mielke","Hanna Wallach","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"02c577382eddda3a8c17afd5facc6144","permalink":"https://rycolab.github.io/publication/zmigrodal-acl-19/","publishdate":"2020-03-13T16:20:07.921794Z","relpermalink":"/publication/zmigrodal-acl-19/","section":"publication","summary":"Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","tags":null,"title":"Counterfactual Data Augmentation for Mitigating Gender Bias in Languages with Rich Morphology","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"7547a9341238e368f994d0782bb943a4","permalink":"https://rycolab.github.io/publication/wucotterell-acl-19/","publishdate":"2020-03-13T16:20:04.817017Z","relpermalink":"/publication/wucotterell-acl-19/","section":"publication","summary":"Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.","tags":null,"title":"Exact Hard Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Arya McCarthy","Damian Blasi","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2aec702aa08c71f0b3cfeb2cc63e90ba","permalink":"https://rycolab.github.io/publication/pimentelal-acl-19/","publishdate":"2020-03-13T16:20:06.292614Z","relpermalink":"/publication/pimentelal-acl-19/","section":"publication","summary":"A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram `gl′ have any systematic relationship to the meaning of words like `glisten′, `gleam′ and `glow′? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small---despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.","tags":null,"title":"Meaning to Form: Measuring Systematicity as Information","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell","Timothy J. O'Donnell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"19ada103f8247e8e1a50fb490f672367","permalink":"https://rycolab.github.io/publication/wual-acl-19/","publishdate":"2020-03-13T16:20:06.933657Z","relpermalink":"/publication/wual-acl-19/","section":"publication","summary":"We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms---providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.","tags":null,"title":"Measuring Morphological Irregularity","type":"publication"},{"authors":["Damian Blasi","Ryan Cotterell","Lawrence Wolf-Sonkin","Sabine Stoll","Balthasar Bickel","Marco Baroni"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"93639342f841bc5e61c57825210d6019","permalink":"https://rycolab.github.io/publication/blasial-acl-19/","publishdate":"2020-03-13T16:20:03.570599Z","relpermalink":"/publication/blasial-acl-19/","section":"publication","summary":"Embedding a clause inside another (``the girl [who likes cars [that run fast]] has arrived″) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.","tags":null,"title":"On the distribution of deep clausal embeddings: A large cross-linguistic study","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"84cc0a9e2fb24ebde73ba952790fd562","permalink":"https://rycolab.github.io/publication/bjervaal-acl-19/","publishdate":"2020-03-13T16:20:02.053931Z","relpermalink":"/publication/bjervaal-acl-19/","section":"publication","summary":"The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.","tags":null,"title":"Uncovering Typological Implications with Belief Nets","type":"publication"},{"authors":["Alexander M. Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"88feb9579b10f90d0805e8ee27e849a8","permalink":"https://rycolab.github.io/publication/hoyleal-acl-19/","publishdate":"2020-03-13T16:20:09.305731Z","relpermalink":"/publication/hoyleal-acl-19/","section":"publication","summary":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","tags":null,"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","type":"publication"},{"authors":["Sabrina Mielke","Ryan Cotterell","Kyle Gorman","Brian Roark","Jason Eisner"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f4b1b6ea089e83dd43526718672dc3d0","permalink":"https://rycolab.github.io/publication/mielkeal-acl-19/","publishdate":"2020-03-13T16:20:01.59522Z","relpermalink":"/publication/mielkeal-acl-19/","section":"publication","summary":"How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that ``translationese″ is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.","tags":null,"title":"What Kind of Language Is Hard to Language-Model?","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"5d53e7002cc0533fd0478a4bc7673df1","permalink":"https://rycolab.github.io/publication/bjervaal-naacl-19/","publishdate":"2020-03-13T16:20:11.946482Z","relpermalink":"/publication/bjervaal-naacl-19/","section":"publication","summary":"In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":null,"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Chaitanya Malaviya","Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"7cfbe19e49c0f05cd5a7b3beaa3f894e","permalink":"https://rycolab.github.io/publication/malaviyanaal-acl-19/","publishdate":"2020-03-13T16:20:10.000326Z","relpermalink":"/publication/malaviyanaal-acl-19/","section":"publication","summary":"English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.","tags":null,"title":"A Simple Joint Model for Improved Contextual Neural Lemmatization","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"dc6d51404dc2effc57d9764c923031ec","permalink":"https://rycolab.github.io/publication/hoyleal-naacl-19/","publishdate":"2020-03-13T16:20:12.842139Z","relpermalink":"/publication/hoyleal-naacl-19/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":null,"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn","Jason Eisner"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"3f8612ffd7dc7fc048ff998b6a81e076","permalink":"https://rycolab.github.io/publication/vylomovaal-naacl-19/","publishdate":"2020-03-13T16:20:14.773742Z","relpermalink":"/publication/vylomovaal-naacl-19/","section":"publication","summary":"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide ''gold'' tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.","tags":null,"title":"Contextualization of Morphological Inflection","type":"publication"},{"authors":["Jieyu Zhao","Tianlu Wang","Mark Yatskar","Ryan Cotterell","Vicente Ordonez","Kai-Wei Chang"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"07cd7213afdbf397a75bca00c039c3ee","permalink":"https://rycolab.github.io/publication/zhoual-naacl-19/","publishdate":"2020-03-13T16:20:13.806807Z","relpermalink":"/publication/zhoual-naacl-19/","section":"publication","summary":"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","tags":null,"title":"Gender Bias in Contextualized Word Embeddings","type":"publication"},{"authors":["Shijia Liu","Adina Williams","Hongyuan Mei","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"06e41617c9ae911725b3fc26bd63b47d","permalink":"https://rycolab.github.io/publication/liual-naacl-19/","publishdate":"2020-03-13T16:20:10.956654Z","relpermalink":"/publication/liual-naacl-19/","section":"publication","summary":"While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.","tags":null,"title":"On the Idiosyncrasies of the Mandarin Chinese Classifier System","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"4df7e71c0378509ff7f0bb1850a5990f","permalink":"https://rycolab.github.io/publication/cotterellal-tacl-19/","publishdate":"2020-03-13T16:20:15.579976Z","relpermalink":"/publication/cotterellal-tacl-19/","section":"publication","summary":"We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language's inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm---how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.","tags":null,"title":"On the Complexity and Typology of Inflectional Morphological Systems","type":"publication"},{"authors":["Sebastian Ruder$^*$","Ryan Cotterell$^*$","Yova Kementchedjhieva","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"2f067d72799894553dc0f72584bb1252","permalink":"https://rycolab.github.io/publication/ruderal-emnlp-18/","publishdate":"2020-03-13T16:20:59.654298Z","relpermalink":"/publication/ruderal-emnlp-18/","section":"publication","summary":"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.","tags":null,"title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","type":"publication"},{"authors":["Yova Kementchedjhieva","Sebastian Ruder","Ryan Cotterell","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"12558fbbd9611f5da65ff93738691b22","permalink":"https://rycolab.github.io/publication/kementchedjhievaal-conll-18/","publishdate":"2020-03-13T16:20:58.333468Z","relpermalink":"/publication/kementchedjhievaal-conll-18/","section":"publication","summary":"Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.","tags":null,"title":"Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction","type":"publication"},{"authors":["Shijie Wu","Pamela Shapiro","Ryan Cotterell"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"898bd87faa75ca4cc9c323a70eb39ebf","permalink":"https://rycolab.github.io/publication/wual-emnlp-18/","publishdate":"2020-03-13T16:21:00.929123Z","relpermalink":"/publication/wual-emnlp-18/","section":"publication","summary":"Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.","tags":null,"title":"Hard Non-Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Arya D. McCarthy","Miikka Silfverberg","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"a150607e03d1cc2b9cc4a3a8da3fad7a","permalink":"https://rycolab.github.io/publication/mccarthyal-udw-18/","publishdate":"2020-03-13T16:21:06.861396Z","relpermalink":"/publication/mccarthyal-udw-18/","section":"publication","summary":"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages---UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project's annotations could be used to validate the other's. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.","tags":null,"title":"Marrying Universal Dependencies and Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Arya D. McCarthy","Katharina Kann","Sabrina Mielke","Garrett Nicolai","Miikka Silfverberg","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"3af314661f7a50d23092c996ac3531de","permalink":"https://rycolab.github.io/publication/cotterellal-conll-18/","publishdate":"2020-03-13T16:20:58.899131Z","relpermalink":"/publication/cotterellal-conll-18/","section":"publication","summary":"The CoNLL-SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages. Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task. This second task featured seven languages. Task 1 received 27 submissions and task 2 received 6 submissions. Both tasks featured a low, medium, and high data condition. Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task. In the inflection task (task 1), 41 of the 52 languages present in last year’s inflection task showed improvement by the best systems in the low-resource setting. The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemmarepeating baseline.","tags":null,"title":"The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection","type":"publication"},{"authors":["Lawrence Wolf-Sonkin$^*$","Jason Naradowsky$^*$","Sabrina J. Mielke$^*$","Ryan Cotterell$^*$"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"81c65001211ead5c4bae2ff60e6a56c7","permalink":"https://rycolab.github.io/publication/wolf-sonkin-acl-18/","publishdate":"2020-03-13T16:20:56.16673Z","relpermalink":"/publication/wolf-sonkin-acl-18/","section":"publication","summary":"Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.","tags":null,"title":"A Structured Variational Autoencoder for Contextual Morphological Inflection","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3ae1326e83220b4c8f5d6afadaa7e6a8","permalink":"https://rycolab.github.io/publication/cotterelleisner-naacl-18/","publishdate":"2020-03-13T16:21:03.020539Z","relpermalink":"/publication/cotterelleisner-naacl-18/","section":"publication","summary":"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information---the first two formant values---rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.","tags":null,"title":"A Deep Generative Model of Vowel Formant Typology","type":"publication"},{"authors":["Ryan Cotterell","Sabrina J. Mielke","Jason Eisner","Brian Roark"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d436f65260058640e7930be367a0e8cb","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-18-a/","publishdate":"2020-03-13T16:21:04.186643Z","relpermalink":"/publication/cotterellal-naacl-18-a/","section":"publication","summary":"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.","tags":null,"title":"Are All Languages Equally Hard to Language-Model?","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Sabrina J. Mielke","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9ee0d90db50ad67fb3060518130a3c42","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-18-b/","publishdate":"2020-03-13T16:21:04.74082Z","relpermalink":"/publication/cotterellal-naacl-18-b/","section":"publication","summary":"Lexical ambiguity makes it difficult to compute useful statistics of a corpus. A given word form might represent any of several morphological feature bundles. One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms. We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones). Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram. We discuss evaluation metrics for this novel task and report results on 5 languages.","tags":null,"title":"Unsupervised Disambiguation of Syncretism in Inflected Lexicons","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sabrina Mielke","Arya McCarthy","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"89cfa7b59e28d9a972a4e49489e18e08","permalink":"https://rycolab.github.io/publication/kiroval-lrec-18/","publishdate":"2020-03-13T16:21:02.270668Z","relpermalink":"/publication/kiroval-lrec-18/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the collection, annotation, and dissemination of project resources since the initial UniMorph release described at LREC 2016.","tags":null,"title":"UniMorph 2.0: Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Julia Kreutzer"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8f9f94018a73e2ff526a65aeedc1d3ec","permalink":"https://rycolab.github.io/publication/cotterellkreutzer-arxiv-18/","publishdate":"2020-03-13T16:20:56.598395Z","relpermalink":"/publication/cotterellkreutzer-arxiv-18/","section":"publication","summary":"","tags":null,"title":"Explaining and Generalizing Back-Translation through Wake-Sleep","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1030c6c628ef205a3f67724e7d809180","permalink":"https://rycolab.github.io/publication/cotterellschuetze-tacl-18/","publishdate":"2020-03-13T16:21:05.340228Z","relpermalink":"/publication/cotterellschuetze-tacl-18/","section":"publication","summary":"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word′s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist′s notion of morphological productivity.","tags":null,"title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1fa421ac33a261ae3d663c5a02cdb76a","permalink":"https://rycolab.github.io/publication/cotterellal-arxiv-18/","publishdate":"2020-03-13T16:20:57.089434Z","relpermalink":"/publication/cotterellal-arxiv-18/","section":"publication","summary":"","tags":null,"title":"On the Diachronic Stability of Irregularity in Inflectional Morphology","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"15717ae85e8cb20882e5431a78d9e060","permalink":"https://rycolab.github.io/publication/kirovcotterell-tacl-18/","publishdate":"2020-03-13T16:21:07.99972Z","relpermalink":"/publication/kirovcotterell-tacl-18/","section":"publication","summary":"Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.","tags":null,"title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate","type":"publication"},{"authors":["Ryan Cotterell","Kevin Duh."],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"1f118c7c6d7e9f331838935ea2fed573","permalink":"https://rycolab.github.io/publication/cotterellduh-ijcnlp-17/","publishdate":"2020-04-20T10:10:30.834491Z","relpermalink":"/publication/cotterellduh-ijcnlp-17/","section":"publication","summary":"Low-resource named entity recognition is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world's languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows knowledge transfer from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.","tags":null,"title":"Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields","type":"publication"},{"authors":["Ryan Cotterell","Georg Heigold"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"4174111fd88ca9bd66c5290666730be9","permalink":"https://rycolab.github.io/publication/cotterellheigold-emnlp-17/","publishdate":"2020-04-20T10:10:31.302764Z","relpermalink":"/publication/cotterellheigold-emnlp-17/","section":"publication","summary":"Even for common NLP tasks, sufficient supervision is not available in many languages--morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.","tags":null,"title":"Cross-lingual, Character-Level Neural Morphological Tagging","type":"publication"},{"authors":["Ryan Cotterell","Ekaterina Vylomova","Huda Khayrallah","Christo Kirov","David Yarowsky"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"d1c98d2aefe8457ea6296afaa5dc2204","permalink":"https://rycolab.github.io/publication/cotterellal-emnlp-17/","publishdate":"2020-04-20T10:10:31.806004Z","relpermalink":"/publication/cotterellal-emnlp-17/","section":"publication","summary":"The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.","tags":null,"title":"Paradigm Completion for Derivational Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c09bf1108b8a9f99886af0418cb4f453","permalink":"https://rycolab.github.io/publication/cotterellal-conll-17/","publishdate":"2020-04-20T10:10:34.320503Z","relpermalink":"/publication/cotterellal-conll-17/","section":"publication","summary":"The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.","tags":null,"title":"CoNLL--SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection in 52 Languages","type":"publication"},{"authors":["Francis Ferraro","Adam Poliak","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"087c51922a795fbc97864e39def05304","permalink":"https://rycolab.github.io/publication/ferraroal-starsem-17/","publishdate":"2020-04-20T10:10:34.797399Z","relpermalink":"/publication/ferraroal-starsem-17/","section":"publication","summary":"We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.","tags":null,"title":"Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"bcf915476f31940a15d842647e3f1001","permalink":"https://rycolab.github.io/publication/kannal-acl-17/","publishdate":"2020-04-20T10:10:33.815305Z","relpermalink":"/publication/kannal-acl-17/","section":"publication","summary":"We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.","tags":null,"title":"One-Shot Neural Cross-Lingual Transfer for Paradigm Completion","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"71b13a4d0772b4d54b40f69f2e93feaf","permalink":"https://rycolab.github.io/publication/cotterelleisner-acl-17/","publishdate":"2020-04-20T10:10:32.755862Z","relpermalink":"/publication/cotterelleisner-acl-17/","section":"publication","summary":"Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.","tags":null,"title":"Probabilistic Typology: Deep Generative Models of Vowel Inventories","type":"publication"},{"authors":["Christo Kirov","John Sylak-Glassman","Rebecca Knowles","Ryan Cotterell","Matt Post"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"cd8133a88495b62b72d13bc2400f38a2","permalink":"https://rycolab.github.io/publication/kiroval-eacl-17/","publishdate":"2020-04-20T10:10:35.714258Z","relpermalink":"/publication/kiroval-eacl-17/","section":"publication","summary":"A traditional claim in linguistics is that all human languages are equally expressive---able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.","tags":null,"title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"88dd0bbfa4129bc86b6c49d1d0fdf629","permalink":"https://rycolab.github.io/publication/vylomovaal-eacl-17/","publishdate":"2020-04-20T10:10:38.652566Z","relpermalink":"/publication/vylomovaal-eacl-17/","section":"publication","summary":"Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.","tags":null,"title":"Context-Aware Prediction of Derivational Word-forms","type":"publication"},{"authors":["Ryan Cotterell","Adam Poliak","Ben Van Durme","Jason Eisner"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"62e68751e7f01daaccf799f3031f75b2","permalink":"https://rycolab.github.io/publication/cotterellalb-eacl-17/","publishdate":"2020-04-20T10:10:37.734145Z","relpermalink":"/publication/cotterellalb-eacl-17/","section":"publication","summary":"The popular skip-gram model induces word embeddings by exploiting the signal from word-context coocurrence. We offer a new interpretation of skip-gram based on exponential family PCA-a form of matrix factorization to generalize the skip-gram model to tensor factorization. In turn, this lets us train embeddings through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show our model improves upon skip-gram.","tags":null,"title":"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis","type":"publication"},{"authors":["Arun Kumar","Ryan Cotterell","Lluís Padró","Antoni Oliver"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"950429cf9d367338236f6c9801bbdeb1","permalink":"https://rycolab.github.io/publication/kumaral-eacl-17/","publishdate":"2020-04-20T10:10:39.619718Z","relpermalink":"/publication/kumaral-eacl-17/","section":"publication","summary":"The Dravidian languages are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a corpus annotated for morphological segmentation and part-of-speech. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.","tags":null,"title":"Morphological Analysis of the Dravidian Language Family","type":"publication"},{"authors":["Ryan Cotterell","John Sylak-Glassman","Christo Kirov"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"fc54a64fe12dd9d57beea964e7ce8876","permalink":"https://rycolab.github.io/publication/cotterellala-eacl-17/","publishdate":"2020-04-20T10:10:36.720999Z","relpermalink":"/publication/cotterellala-eacl-17/","section":"publication","summary":"Many of the world's languages contain an abundance of inflected forms for each lexeme. A critical task in processing such languages is predicting these inflected forms. We develop a novel statistical model for the problem, drawing on graphical modeling techniques and recent advances in deep learning. We derive a Metropolis-Hastings algorithm to jointly decode the model. Our Bayesian network draws inspiration from principal parts morphological analysis. We demonstrate improvements on 5 languages.","tags":null,"title":"Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"919462c9664194568756fd3006526dba","permalink":"https://rycolab.github.io/publication/kannal-eacl-17/","publishdate":"2020-04-20T10:10:40.526493Z","relpermalink":"/publication/kannal-eacl-17/","section":"publication","summary":"We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.","tags":null,"title":"Neural Multi-Source Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Arun Kumar","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"a39aaa943290d5f6face02f3d9d47983","permalink":"https://rycolab.github.io/publication/cotterellal-emnlp-16/","publishdate":"2020-04-20T10:09:57.258238Z","relpermalink":"/publication/cotterellal-emnlp-16/","section":"publication","summary":"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure---especially in the case of derivational morphology. In this work, we introduce a discriminative joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.","tags":null,"title":"Morphological Segmentation Inside-Out","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1feb9c000aedb9cbb4a319955397bc86","permalink":"https://rycolab.github.io/publication/kannal-emnlp-16/","publishdate":"2020-04-20T10:09:57.660609Z","relpermalink":"/publication/kannal-emnlp-16/","section":"publication","summary":"Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian.","tags":null,"title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments","type":"publication"},{"authors":["Tim Vieira$^*$","Ryan Cotterell$^*$","Jason Eisner"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1906bb6181348303c0d677575abad1ce","permalink":"https://rycolab.github.io/publication/vieiraal-emnlp-16/","publishdate":"2020-04-20T10:09:58.396743Z","relpermalink":"/publication/vieiraal-emnlp-16/","section":"publication","summary":"We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2–6x speedup over a baseline, with no significant drop in accuracy.","tags":null,"title":"Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze","Jason Eisner"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"d5e258b2cfce483a0a8e30a31119cd1d","permalink":"https://rycolab.github.io/publication/cotterellal-acl-16/","publishdate":"2020-04-20T10:09:59.062781Z","relpermalink":"/publication/cotterellal-acl-16/","section":"publication","summary":"Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word’s component morphemes. We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity","tags":null,"title":"Morphological Smoothing and Extrapolation of Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"0dae3427f8a795d86396e714ddca0095","permalink":"https://rycolab.github.io/publication/cotterellal-sigmorphon-16/","publishdate":"2020-04-20T10:09:59.656993Z","relpermalink":"/publication/cotterellal-sigmorphon-16/","section":"publication","summary":"The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best—in fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.","tags":null,"title":"The SIGMORPHON 2016 Shared Task—Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Tim Vieira","Hinrich Schütze"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"7fa0b09b73010847f89c913ed7de741a","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-16/","publishdate":"2020-04-20T10:10:00.37066Z","relpermalink":"/publication/cotterellal-naacl-16/","section":"publication","summary":"We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest 7 → fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest 7 → funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.","tags":null,"title":"A Joint Model of Orthography and Morphological Segmentation","type":"publication"},{"authors":["Pushpendre Rastogi","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"bffcb6da09c3c39cf85ea1db79e3a5b8","permalink":"https://rycolab.github.io/publication/rastogial-naacl-16/","publishdate":"2020-04-20T10:10:00.914565Z","relpermalink":"/publication/rastogial-naacl-16/","section":"publication","summary":"How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization.","tags":null,"title":"Weighting Finite-State Transductions With Neural Context","type":"publication"},{"authors":["Nanyun Peng","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"27421098a0f9e3c451ea02557297cb9c","permalink":"https://rycolab.github.io/publication/pengal-emnlp-15/","publishdate":"2020-04-20T10:10:21.461087Z","relpermalink":"/publication/pengal-emnlp-15/","section":"publication","summary":"We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.  This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.","tags":null,"title":"Dual Decomposition Inference for Graphical Models over Strings","type":"publication"},{"authors":["Thomas Müller","Ryan Cotterell","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"307e76fe83844df78b8722e4c4925865","permalink":"https://rycolab.github.io/publication/muelleral-conll-15/","publishdate":"2020-04-20T10:10:21.912734Z","relpermalink":"/publication/muelleral-conll-15/","section":"publication","summary":"We present Lemming, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. Lemming sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial.","tags":null,"title":"Joint Lemmatization and Morphological Tagging with Lemming","type":"publication"},{"authors":["Ryan Cotterell","Thomas Müller","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"457b1282971eef22ec6477186cc6f4c1","permalink":"https://rycolab.github.io/publication/cotterellal-conll-15/","publishdate":"2020-04-20T10:10:22.689806Z","relpermalink":"/publication/cotterellal-conll-15/","section":"publication","summary":"We present labeled morphological segmentation—an alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy of morphotactic tagsets and Chipmunk, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii)  morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline.","tags":null,"title":"Labeled Morphological Segmentation with Semi-Markov Models","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"58c9c1b17d1c3c1e7deb1d36d1765e88","permalink":"https://rycolab.github.io/publication/cotterellschuetze-naacl-15/","publishdate":"2020-04-20T10:10:25.459218Z","relpermalink":"/publication/cotterellschuetze-naacl-15/","section":"publication","summary":"Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.","tags":null,"title":"Morphological Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"db153a927a072c4049d322aa1f4dfc77","permalink":"https://rycolab.github.io/publication/cotterelleisner-naacl-15/","publishdate":"2020-04-20T10:10:24.471872Z","relpermalink":"/publication/cotterelleisner-naacl-15/","section":"publication","summary":"We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy.","tags":null,"title":"Penalized Expectation Propagation for Graphical Models over Strings","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"585f26f2d387f8189154c38ea4fd00a6","permalink":"https://rycolab.github.io/publication/cotterellal-tacl-15/","publishdate":"2020-04-20T10:10:23.509194Z","relpermalink":"/publication/cotterellal-tacl-15/","section":"publication","summary":"The observed pronunciations or spellings of words are often explained as arising from the ''underlying forms'' of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.","tags":null,"title":"Modeling Word Forms Using Latent Underlying Morphs and Phonology","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"98faa9c135408e6a0e6802b0a788931b","permalink":"https://rycolab.github.io/publication/cotterellal-acl-14/","publishdate":"2020-04-20T10:10:54.993291Z","relpermalink":"/publication/cotterellal-acl-14/","section":"publication","summary":"String similarity is most often measured by weighted or unweighted edit distance d(x, y). Ristad and Yianilos (1998) defined stochastic edit distance—a probability distribution p(y | x) whose parameters can be trained from data. We generalize this so that the probability of choosing each edit operation can depend on contextual features. We show how to construct and train a probabilistic finite-state transducer that computes our stochastic  ontextual edit distance. To illustrate the improvement from conditioning on context, we model typos found in social media text.","tags":null,"title":"Stochastic Contextual Edit Distance and Probabilistic FSTs","type":"publication"}]